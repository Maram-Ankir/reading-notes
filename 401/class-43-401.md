# Ethics in the workplace

### The code I’m still ashamed of

If you write code for a living, there’s a chance that at some point in your career, someone will ask you to code something a little deceitful – if not outright unethical. 

### Project Dragonfly, Google’s censored search engine

Google is experiencing a “moral and ethical” crisis. That’s the view of hundreds of employees at the tech company, who are protesting the development of a censored search engine for internet users in China.
About 1,400 Google employees — out of the more than 88,000 — signed a letter to company executives this week, seeking more details and transparency about the project and demanding employee input in decisions about what kind of work Google takes on. They also expressed concern that the company is violating its own ethical principles.

### Amazon workers demand Jeff Bezos cancel “Recognition” software

Amazon employees objected to the Trump administration’s “zero-tolerance” policy at the U.S. border, which has resulted in thousands of children being separated from their parents.
“Along with much of the world we watched in horror recently as U.S. authorities tore children away from their parents,” the letter, distributed on a mailing list called ‘we-won’t-build-it,’ states. “In the face of this immoral U.S. policy, and the U.S.’s increasingly inhumane treatment of refugees and immigrants beyond this specific policy, we are deeply concerned that Amazon is implicated, providing infrastructure and services that enable ICE and DHS.”

### Google and AI

Google is committing to not using artificial intelligence for weapons or surveillance after employees protested the company’s involvement in Project Maven, a Pentagon pilot program that uses artificial intelligence to analyze drone footage. However, Google says it will continue to work with the United States military on cybersecurity, search and rescue, and other non-offensive projects.

_____________________________________________________________________________

 
# Tech Company Principles

### Microsoft AI Principles
We put our responsible AI principles into practice through the Office of Responsible AI (ORA), the AI, Ethics, and Effects in Engineering and Research (Aether) Committee, and Responsible AI Strategy in Engineering (RAISE). The Aether Committee advises our leadership on the challenges and opportunities presented by AI innovations. ORA sets our rules and governance processes, working closely with teams across the company to enable the effort. RAISE is a team that enables the implementation of Microsoft responsible AI rules across engineering groups.

### Ethical OS Toolkit
As technologists, it’s only natural that we spend most of our time focusing on how our tech will change the world for the better. Which is great. Everyone loves a sunny disposition. But perhaps it’s more useful, in some ways, to consider the glass half empty. What if, in addition to fantasizing about how our tech will save the world, we spent some time dreading all the ways it might, possibly, perhaps, just maybe, screw everything up? No one can predict exactly what tomorrow will bring (though somewhere in the tech world, someone is no doubt working on it). So until we get that crystal ball app, the best we can hope to do is anticipate the long-term social impact and unexpected uses of the tech we create today.

### Google AI Principles

#### Objectives for AI applications 

1-Be socially beneficial.
2-Avoid creating or reinforcing unfair bias.
3-Be built and tested for safety.
4-Be accountable to people.
5-Incorporate privacy design principles.
6-Uphold high standards of scientific excellence.
7-Be made available for uses that accord with these principles.

#### AI applications we will not pursue
- Technologies that cause or are likely to cause overall harm. Where there is a material risk of harm, we will proceed only where we believe that the benefits substantially outweigh the risks, and will incorporate appropriate safety constraints.

- Weapons or other technologies whose principal purpose or implementation is to cause or directly facilitate injury to people.

- Technologies that gather or use information for surveillance violating internationally accepted norms.

- Technologies whose purpose contravenes widely accepted principles of international law and human rights.

#### AI for the long term

While this is how we’re choosing to approach AI, we understand there is room for many voices in this conversation. As AI technologies progress, we’ll work with a range of stakeholders to promote thoughtful leadership in this area, drawing on scientifically rigorous and multidisciplinary approaches. And we will continue to share what we’ve learned to improve AI technologies and practices.